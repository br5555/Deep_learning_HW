{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TensorFlow with GPU",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "/v2/external/notebooks/gpu.ipynb",
          "timestamp": 1523881205188
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "BlmQIFSLZDdc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Confirm TensorFlow can see the GPU\n",
        "\n",
        "Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P)."
      ]
    },
    {
      "metadata": {
        "id": "3IEVK-KFxi5Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0940e8b1-5283-4b2d-e3e6-8853f3deb804"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QXRh0DPiZRyG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a typical convolutional neural network layer over a\n",
        "random image and manually places the resulting ops on either the CPU or the GPU\n",
        "to compare execution speed."
      ]
    },
    {
      "metadata": {
        "id": "t9ALbbpmY9rm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 2425
        },
        "outputId": "3bbc99e2-bc74-43d0-dd21-d2fba2b55d1a",
        "executionInfo": {
          "status": "error",
          "timestamp": 1523836340490,
          "user_tz": -120,
          "elapsed": 2708,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  random_image_cpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\n",
        "  net_cpu = tf.reduce_sum(net_cpu)\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n",
        "  net_gpu = tf.reduce_sum(net_gpu)\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "# Test execution once to detect errors early.\n",
        "try:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise\n",
        "\n",
        "def cpu():\n",
        "  sess.run(net_cpu)\n",
        "  \n",
        "def gpu():\n",
        "  sess.run(net_gpu)\n",
        "  \n",
        "# Runs the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "This error most likely means that this notebook is not configured to use a GPU.  Change this in Notebook Settings via the command palette (cmd/ctrl-shift-P) or the Edit menu.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-055975cd90d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Test execution once to detect errors early.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   print(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'conv2d_1/bias': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: conv2d_1/bias = VariableV2[_class=[\"loc:@conv2d_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[32], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\n\nCaused by op u'conv2d_1/bias', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-055975cd90d7>\", line 15, in <module>\n    net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.py\", line 614, in conv2d\n    return layer.apply(inputs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 809, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 680, in __call__\n    self.build(input_shapes)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.py\", line 151, in build\n    dtype=self.dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 533, in add_variable\n    partitioner=partitioner)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1297, in get_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1093, in get_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 439, in get_variable\n    constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 408, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 800, in _get_single_variable\n    use_resource=use_resource)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2157, in variable\n    use_resource=use_resource)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2147, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2130, in default_variable_creator\n    constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 233, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 134, in variable_op_v2\n    shared_name=shared_name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 1043, in _variable_v2\n    shared_name=shared_name, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'conv2d_1/bias': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: conv2d_1/bias = VariableV2[_class=[\"loc:@conv2d_1/bias\"], container=\"\", dtype=DT_FLOAT, shape=[32], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "FAhyiW7NuSKY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "c5eaa927-fa08-41d7-8eb5-62e0330759fd"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import math\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import skimage as ski\n",
        "import skimage.io\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class TFConv2d:\n",
        "    \n",
        "    def __init__(self,width=1, height=1, channels=1):\n",
        "        \n",
        "        height = 28\n",
        "        width = 28\n",
        "        channels = 1\n",
        "        n_inputs = height * width\n",
        "        \n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.channels = channels\n",
        "        \n",
        "        self.conv1_fmaps = 16\n",
        "        self.conv1_ksize = 5\n",
        "        self.conv1_stride = 1\n",
        "        self.conv1_pad = \"SAME\"\n",
        "        \n",
        "        self.conv2_fmaps = 32\n",
        "        self.conv2_ksize = 5\n",
        "        self.conv2_stride = 1\n",
        "        self.conv2_pad = \"SAME\"\n",
        "        \n",
        "        self.pool3_fmaps = self.conv2_fmaps\n",
        "        self.learning_rate = 0.1\n",
        "\n",
        "        self.n_fc1 = 512\n",
        "        self.n_outputs = 10\n",
        "        \n",
        "        self.n_inputs = width * height\n",
        "        \n",
        "        print(height, width, channels)\n",
        "        with tf.name_scope(\"model\"):\n",
        "          self.X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
        "          self.X_reshaped = tf.reshape(self.X, shape=[-1, height, width, channels])\n",
        "          self.y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
        "\n",
        "          self.l2_reg = 0.001\n",
        "          self.regularizer = tf.contrib.layers.l2_regularizer(self.l2_reg)\n",
        "\n",
        "          self.conv1 = tf.layers.conv2d(self.X_reshaped, filters= self.conv1_fmaps, kernel_size = self.conv1_ksize\n",
        "                                  ,strides = self.conv1_stride, padding = self.conv1_pad, kernel_regularizer = self.regularizer,\n",
        "                                  activation=None, name=\"my_conv1\")\n",
        "          self.max_pool1 = tf.nn.max_pool(self.conv1, ksize=[1,2,2,1], strides=[1,2,2,1],padding=\"VALID\")\n",
        "\n",
        "          self.relu1 = tf.nn.relu(self.max_pool1)\n",
        "\n",
        "          self.conv2 = tf.layers.conv2d(self.relu1, filters=self.conv2_fmaps, kernel_size=self.conv2_ksize,\n",
        "                                  strides=self.conv2_stride, padding=self.conv2_pad, kernel_regularizer = self.regularizer,\n",
        "                                  activation=None, name=\"my_conv2\")\n",
        "\n",
        "          self.max_pool2 = tf.nn.max_pool(self.conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"VALID\")\n",
        "\n",
        "          self.relu2 = tf.nn.relu(self.max_pool2)\n",
        "\n",
        "          self.pool2_flattern = tf.reshape(self.relu2, shape=[-1, self.pool3_fmaps * 7 * 7])\n",
        "\n",
        "          self.fc1 = tf.layers.dense(self.pool2_flattern, self.n_fc1, activation=tf.nn.relu,kernel_regularizer = self.regularizer,\n",
        "                                    name=\"fc\")\n",
        "\n",
        "          self.logits = tf.layers.dense(self.fc1, self.n_outputs, name=\"output\")\n",
        "          self.Y_proba = tf.nn.softmax(self.logits, name=\"Y_proba\")\n",
        "\n",
        "          self.xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
        "                                                                    labels=self.y)\n",
        "          self.loss = tf.reduce_mean(self.xentropy)\n",
        "          self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "          self.training_op = self.optimizer.minimize(self.loss)\n",
        "\n",
        "          self.correct = tf.nn.in_top_k(self.logits, self.y, 1)\n",
        "          self.accuracy = tf.reduce_mean(tf.cast(self.correct, tf.float32))\n",
        "\n",
        "          self.sess = tf.Session()\n",
        "          self.init = tf.global_variables_initializer()\n",
        "          self.saver = tf.train.Saver()\n",
        "    \n",
        "    def evaluate(self, X_eval, y_true, my_title=\"train\"):\n",
        "        print(\"Usao sam\")\n",
        "        y_pred = np.argmax(self.prediction(X_eval),axis=1)\n",
        "        cnf_matrix = confusion_matrix(y_true, y_pred)\n",
        "        cnn_recall = recall_score(y_true, y_pred, average='micro')\n",
        "        cnn_precision = precision_score(y_true, y_pred, average='micro')\n",
        "        cnn_acc = accuracy_score(y_true, y_pred)\n",
        "        print(my_title)\n",
        "        print(cnf_matrix)\n",
        "        return  cnn_acc, cnn_precision, cnn_recall\n",
        "    \n",
        "    def prediction(self, X_eval):\n",
        "        sess = self.sess\n",
        "#             self.saver.restore(sess, \"./batch_conv2d_para.ckpt\")\n",
        "#         sess.run(self.init)\n",
        "        return sess.run(self.Y_proba, feed_dict={self.X: X_eval})\n",
        "    \n",
        "    def draw_conv_filters(self, weights):\n",
        "        w = weights.copy()\n",
        "        num_filters = w.shape[3]\n",
        "        num_channels = w.shape[2]\n",
        "        k = w.shape[0]\n",
        "        assert w.shape[0] == w.shape[1]\n",
        "        w = w.reshape(k, k, num_channels, num_filters)\n",
        "        w -= w.min()\n",
        "        w /= w.max()\n",
        "        border = 1\n",
        "        cols = 8\n",
        "        rows = math.ceil(num_filters / cols)\n",
        "        width = cols * k + (cols-1) * border\n",
        "        height = rows * k + (rows-1) * border\n",
        "        img = np.zeros([np.int(height), np.int(width), np.int(num_channels)])\n",
        "        for i in range(np.int(num_filters)):\n",
        "            r = int(i / cols) * (k + border)\n",
        "            c = int(i % cols) * (k + border)\n",
        "            img[r:r+k,c:c+k,:] = w[:,:,:,i]\n",
        "#         img.astype(np.uint8)\n",
        "#         ski.io.imshow(img)\n",
        "#         ski.io.show()\n",
        "        print(\"Shape is \", img.shape)\n",
        "        imgplot = plt.imshow(img.squeeze())\n",
        "        plt.show()\n",
        "  \n",
        "    \n",
        "    def visual_con(self):\n",
        "        conv1_var = tf.contrib.framework.get_variables('my_conv1/kernel:0')[0]\n",
        "        conv1_weights = conv1_var.eval(session=self.sess)\n",
        "        self.draw_conv_filters(conv1_weights)\n",
        "    \n",
        "    \n",
        "    def top_3(self, X_train, y_train, niter=1, batch_size = 1):\n",
        "        sess = self.sess\n",
        "        \n",
        "        n, N= X_train.shape\n",
        "        s = np.arange(n)\n",
        "        \n",
        "        ako = True\n",
        "        \n",
        "        \n",
        "        max_loss1 = 0\n",
        "        max_loss2 = 0\n",
        "        max_loss3 = 0\n",
        "        X1 = None\n",
        "        X2 = None\n",
        "        X3 = None\n",
        "        y1 = None\n",
        "        y2 = None\n",
        "        y3 = None\n",
        "        \n",
        "        if ako:\n",
        "            sess.run(self.init)\n",
        "    \n",
        "            for i in range(niter):\n",
        "            \n",
        "                n_batches = n // batch_size\n",
        "                np.random.shuffle(s)\n",
        "                X_train = X_train[s]\n",
        "                y_train = y_train[s]\n",
        "                \n",
        "                for j in range(n_batches):\n",
        "                    X_batch = X_train[j*batch_size: (j+1)*batch_size]\n",
        "                    y_batch = y_train[j*batch_size: (j+1)*batch_size]\n",
        "                    \n",
        "                    loss = sess.run([ self.loss], feed_dict={self.X: X_batch, self.y: y_batch})\n",
        "                    \n",
        "                    if(loss > max_loss1 and i==(niter-1)):\n",
        "                        X1 = X_batch\n",
        "                        y1 = y_batch\n",
        "                        max_loss1 = loss\n",
        "                    elif(loss > max_loss2  and i==(niter-1)):\n",
        "                        X2 = X_batch\n",
        "                        y2 = y_batch\n",
        "                        max_loss2 = loss\n",
        "                    elif(loss > max_loss3  and i==(niter-1)):\n",
        "                        X3 = X_batch\n",
        "                        y3 = y_batch\n",
        "                        max_loss3 = loss\n",
        "\n",
        "            print(\"First loss is\",X1)\n",
        "            print(\"Predictio is\", self.prediction(X1))\n",
        "            print(\"Real class is\", y1)\n",
        "            print(\"First loss is\",X2)\n",
        "            print(\"Predictio is\", self.prediction(X2))\n",
        "            print(\"Real class is\", y2)\n",
        "            print(\"First loss is\",X3)\n",
        "            print(\"Predictio is\", self.prediction(X3))\n",
        "            print(\"Real class is\", y3)\n",
        "    \n",
        "    \n",
        "  \n",
        "    def batch_train(self, X, y, niter=10000, batch_size = 50):\n",
        "        sess = self.sess\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0, test_size=0.1)\n",
        "        \n",
        "        n, N= X_train.shape\n",
        "        s = np.arange(n)\n",
        "        \n",
        "        ako = True\n",
        "        acc_train = []\n",
        "        recall_train = []\n",
        "        precision_train = []\n",
        "        acc_valid = []\n",
        "        recall_valid = []\n",
        "        precision_valid = []\n",
        "        \n",
        "        max_loss1 = 0\n",
        "        max_loss2 = 0\n",
        "        max_loss3 = 0\n",
        "        X1 = None\n",
        "        X2 = None\n",
        "        X3 = None\n",
        "        y1 = None\n",
        "        y2 = None\n",
        "        y3 = None\n",
        "        \n",
        "        loss_s = []\n",
        "        if ako:\n",
        "            sess.run(self.init)\n",
        "    \n",
        "            for i in range(niter):\n",
        "                loss_uk = 0\n",
        "                n_batches = n // batch_size\n",
        "                np.random.shuffle(s)\n",
        "                X_train = X_train[s]\n",
        "                y_train = y_train[s]\n",
        "                \n",
        "                for j in range(n_batches):\n",
        "                    X_batch = X_train[j*batch_size: (j+1)*batch_size]\n",
        "                    y_batch = y_train[j*batch_size: (j+1)*batch_size]\n",
        "                    \n",
        "                    _, loss = sess.run([self.training_op, self.loss], feed_dict={self.X: X_batch, self.y: y_batch})\n",
        "                    loss_uk = loss_uk +loss\n",
        "                    if(loss > max_loss1 and i==(niter-1)):\n",
        "                        X1 = X_batch\n",
        "                        y1 = y_batch\n",
        "                        max_loss1 = loss \n",
        "                    elif(loss > max_loss2  and i==(niter-1)):\n",
        "                        X2 = X_batch\n",
        "                        y2 = y_batch\n",
        "                        max_loss2 = loss\n",
        "                    elif(loss > max_loss3  and i==(niter-1)):\n",
        "                        X3 = X_batch\n",
        "                        y3 = y_batch\n",
        "                        max_loss3 = loss\n",
        "\n",
        "                loss_s +=[loss_uk]\n",
        "                cnn_acc_train, cnn_precision_train, cnn_recall_train= self.evaluate(X_train, y_train)\n",
        "                acc_train += [cnn_acc_train]\n",
        "                recall_train += [cnn_recall_train]\n",
        "                precision_train += [cnn_precision_train]\n",
        "                cnn_acc_valid, cnn_precision_valid, cnn_recall_valid= self.evaluate(X_valid, y_valid, my_title=\"valid\")\n",
        "                acc_valid += [cnn_acc_valid]\n",
        "                recall_valid += [cnn_recall_valid]\n",
        "                precision_valid += [cnn_precision_valid]\n",
        "            self.visual_con()\n",
        "            print(\"First loss is\",X1)\n",
        "            print(\"Predictio is\", self.prediction(X1))\n",
        "            print(\"Real class is\", y1)\n",
        "            print(\"First loss is\",X2)\n",
        "            print(\"Predictio is\", self.prediction(X2))\n",
        "            print(\"Real class is\", y2)\n",
        "            print(\"First loss is\",X3)\n",
        "            print(\"Predictio is\", self.prediction(X3))\n",
        "            print(\"Real class is\", y3)\n",
        "        fig, axes = plt.subplots(4,1,figsize=(20, 10))\n",
        "        axes[0].plot(np.linspace(1.0, niter,num=niter), acc_train, label='accuracy train')\n",
        "        axes[0].legend(loc='best')\n",
        "        axes[0].set_xlabel('number of iteration')\n",
        "\n",
        "\n",
        "\n",
        "        axes[1].plot(np.linspace(1.0, niter,num=niter), recall_train, label= 'recall train')\n",
        "\n",
        "\n",
        "        axes[1].legend(loc='best')\n",
        "        axes[1].set_xlabel('number of iteration')\n",
        "        \n",
        "        axes[2].plot(np.linspace(1.0, niter,num=niter), cnn_precision_train, label='precision train')\n",
        "        axes[2].legend(loc='best')\n",
        "        axes[2].set_xlabel('number of iteration')\n",
        "        \n",
        "        axes[0].plot(np.linspace(1.0, niter,num=niter), acc_valid, label='accuracy valid')\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        axes[1].plot(np.linspace(1.0, niter,num=niter), recall_valid, label= 'recall valid')\n",
        "        \n",
        "        axes[2].plot(np.linspace(1.0, niter,num=niter), cnn_precision_valid, label='precision valid')\n",
        "        \n",
        "        axes[3].plot(np.linspace(1.0, niter,num=niter), loss_s, label='loss')\n",
        "        axes[3].legend(loc='best')\n",
        "        axes[3].set_xlabel('number of iteration')\n",
        "\n",
        "        plt.show()\n",
        "    def pokreni_me(self):\n",
        "      n_epochs = 10\n",
        "      batch_size = 100\n",
        "\n",
        "      with tf.Session() as sess:\n",
        "          sess.run(self.init)\n",
        "          for epoch in range(n_epochs):\n",
        "              for iteration in range(mnist.train.num_examples // batch_size):\n",
        "                  X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "                  sess.run(self.training_op, feed_dict={self.X: X_batch, self.y: y_batch})\n",
        "              acc_train = accuracy.eval(feed_dict={self.X: X_batch, self.y: y_batch})\n",
        "              acc_test = accuracy.eval(feed_dict={self.X: mnist.test.images, self.y: mnist.test.labels})\n",
        "              print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
        "\n",
        "              save_path = saver.save(sess, \"./my_mnist_model\")\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
        "    my_CNN = TFConv2d(height=28, width=28)\n",
        "    my_CNN.batch_train(mnist.train.images, mnist.train.labels, niter=2, batch_size = 50)\n",
        "    my_CNN.top_3(mnist.train.images, mnist.train.labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "(28, 28, 1)\n",
            "Usao sam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n9p1sCBZySC-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1889
        },
        "outputId": "d13921ca-3ec5-45ce-d267-e70f39f37d05",
        "executionInfo": {
          "status": "error",
          "timestamp": 1523837976714,
          "user_tz": -120,
          "elapsed": 1242,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          }
        }
      },
      "cell_type": "code",
      "source": [
        "my_CNN = TFConv2d(height=28, width=28)\n",
        "my_CNN.pokreni_me()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-86357bd0fa9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_CNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_CNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpokreni_me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-a1666bd7411c>\u001b[0m in \u001b[0;36mpokreni_me\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                   \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                   \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m               \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m               \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 156800 values, but the requested shape requires a multiple of 25088\n\t [[Node: Reshape_1 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu_1, Reshape_1/shape)]]\n\nCaused by op u'Reshape_1', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-22-86357bd0fa9d>\", line 1, in <module>\n    my_CNN = TFConv2d(height=28, width=28)\n  File \"<ipython-input-21-a1666bd7411c>\", line 61, in __init__\n    self.pool2_flattern = tf.reshape(self.relu2, shape=[-1, self.pool3_fmaps * self.height * self.width])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3903, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 156800 values, but the requested shape requires a multiple of 25088\n\t [[Node: Reshape_1 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu_1, Reshape_1/shape)]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "QkX-kdQPv3j-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "height = 28\n",
        "width = 28\n",
        "channels = 1\n",
        "n_inputs = height * width\n",
        "\n",
        "conv1_fmaps = 32\n",
        "conv1_ksize = 3\n",
        "conv1_stride = 1\n",
        "conv1_pad = \"SAME\"\n",
        "\n",
        "conv2_fmaps = 64\n",
        "conv2_ksize = 3\n",
        "conv2_stride = 2\n",
        "conv2_pad = \"SAME\"\n",
        "\n",
        "pool3_fmaps = conv2_fmaps\n",
        "\n",
        "n_fc1 = 64\n",
        "n_outputs = 10\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.name_scope(\"inputs\"):\n",
        "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
        "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
        "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
        "\n",
        "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
        "                         strides=conv1_stride, padding=conv1_pad,\n",
        "                         activation=tf.nn.relu, name=\"conv1\")\n",
        "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
        "                         strides=conv2_stride, padding=conv2_pad,\n",
        "                         activation=tf.nn.relu, name=\"conv2\")\n",
        "\n",
        "with tf.name_scope(\"pool3\"):\n",
        "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
        "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
        "\n",
        "with tf.name_scope(\"fc1\"):\n",
        "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
        "\n",
        "with tf.name_scope(\"output\"):\n",
        "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
        "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
        "    loss = tf.reduce_mean(xentropy)\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "with tf.name_scope(\"init_and_save\"):\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZjVyy2Vgv54y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a32909f1-1c81-433b-9848-43d183ab779c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523836804166,
          "user_tz": -120,
          "elapsed": 878,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uYDLs6g8wFdY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 995
        },
        "outputId": "854954af-551e-49e1-df24-a2cebdc40092",
        "executionInfo": {
          "status": "error",
          "timestamp": 1523837306426,
          "user_tz": -120,
          "elapsed": 484244,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          }
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for iteration in range(mnist.train.num_examples // batch_size):\n",
        "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
        "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
        "\n",
        "        save_path = saver.save(sess, \"./my_mnist_model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 'Train accuracy:', 0.96, 'Test accuracy:', 0.9823)\n",
            "(1, 'Train accuracy:', 0.97, 'Test accuracy:', 0.9862)\n",
            "(2, 'Train accuracy:', 0.99, 'Test accuracy:', 0.9859)\n",
            "(3, 'Train accuracy:', 1.0, 'Test accuracy:', 0.987)\n",
            "(4, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9897)\n",
            "(5, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9891)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-358c24466d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IV4EgJlO03VG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "height = 28\n",
        "width = 28\n",
        "channels = 1\n",
        "n_inputs = height * width\n",
        "        \n",
        "\n",
        "        \n",
        "conv1_fmaps = 16\n",
        "conv1_ksize = 5\n",
        "conv1_stride = 1\n",
        "conv1_pad = \"SAME\"\n",
        "\n",
        "conv2_fmaps = 32\n",
        "conv2_ksize = 5\n",
        "conv2_stride = 1\n",
        "conv2_pad = \"SAME\"\n",
        "        \n",
        "pool3_fmaps = conv2_fmaps\n",
        "learning_rate = 0.001\n",
        "\n",
        "n_fc1 = 512\n",
        "n_outputs = 10\n",
        "        \n",
        "n_inputs = width * height\n",
        "   \n",
        "  \n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
        "X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
        "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
        "\n",
        "l2_reg = 0.001\n",
        "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
        "               \n",
        "conv1 = tf.layers.conv2d(X_reshaped, filters= conv1_fmaps, kernel_size = conv1_ksize\n",
        "                                ,strides = conv1_stride, padding = conv1_pad, kernel_regularizer = regularizer,\n",
        "                                activation=None, name=\"my_conv1\")\n",
        "max_pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1],padding=\"VALID\")\n",
        "        \n",
        "relu1 = tf.nn.relu(max_pool1)\n",
        "        \n",
        "conv2 = tf.layers.conv2d(relu1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
        "                                strides=conv2_stride, padding=conv2_pad, kernel_regularizer = regularizer,\n",
        "                                activation=None, name=\"my_conv2\")\n",
        "\n",
        "\n",
        "\n",
        "# with tf.name_scope(\"pool3\"):\n",
        "#     pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
        "#     pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
        "\n",
        "max_pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"VALID\")\n",
        "        \n",
        "relu2 = tf.nn.relu(max_pool2)\n",
        "        \n",
        "pool2_flattern = tf.reshape(relu2, shape=[-1, pool3_fmaps * 7 * 7])\n",
        "        \n",
        "fc1 = tf.layers.dense(pool2_flattern, n_fc1, activation=tf.nn.relu,kernel_regularizer = regularizer,\n",
        "                                  name=\"fc\")\n",
        "        \n",
        "logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
        "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
        "        \n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                                  labels=y)\n",
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "        \n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "        \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5dwe0q7XwKEa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "79c2469c-9b30-446c-bfac-80c989f32d65",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523840781804,
          "user_tz": -120,
          "elapsed": 786832,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          }
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for iteration in range(mnist.train.num_examples // batch_size):\n",
        "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
        "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
        "\n",
        "        save_path = saver.save(sess, \"./my_mnist_model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 'Train accuracy:', 0.99, 'Test accuracy:', 0.9797)\n",
            "(1, 'Train accuracy:', 0.98, 'Test accuracy:', 0.9882)\n",
            "(2, 'Train accuracy:', 0.99, 'Test accuracy:', 0.9863)\n",
            "(3, 'Train accuracy:', 0.99, 'Test accuracy:', 0.9878)\n",
            "(4, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9921)\n",
            "(5, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9877)\n",
            "(6, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9917)\n",
            "(7, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9926)\n",
            "(8, 'Train accuracy:', 1.0, 'Test accuracy:', 0.9911)\n",
            "(9, 'Train accuracy:', 1.0, 'Test accuracy:', 0.99)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d4ML63bLqUQm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "ccb16794-39d3-4c29-d75f-2d1418900a98",
        "executionInfo": {
          "status": "error",
          "timestamp": 1523886182846,
          "user_tz": -120,
          "elapsed": 2646,
          "user": {
            "displayName": "Branko Radoš",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100447339136947181118"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import math\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import skimage as ski\n",
        "import skimage.io\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "DATA_DIR = \"./\"\n",
        "SAVE_DIR = \"./\"\n",
        "\n",
        "\n",
        "def shuffle_data(data_x, data_y):\n",
        "  indices = np.arange(data_x.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  shuffled_data_x = np.ascontiguousarray(data_x[indices])\n",
        "  shuffled_data_y = np.ascontiguousarray(data_y[indices])\n",
        "  return shuffled_data_x, shuffled_data_y\n",
        "\n",
        "def unpickle(file):\n",
        "  fo = open(file, 'rb')\n",
        "  dict = pickle.load(fo, encoding='latin1')\n",
        "  fo.close()\n",
        "  return dict\n",
        "\n",
        "\n",
        "def draw_conv_filters(epoch, step, weights, save_dir):\n",
        "  w = weights.copy()\n",
        "  num_filters = w.shape[3]\n",
        "  num_channels = w.shape[2]\n",
        "  k = w.shape[0]\n",
        "  assert w.shape[0] == w.shape[1]\n",
        "  w = w.reshape(k, k, num_channels, num_filters)\n",
        "  w -= w.min()\n",
        "  w /= w.max()\n",
        "  border = 1\n",
        "  cols = 8\n",
        "  rows = math.ceil(num_filters / cols)\n",
        "  width = cols * k + (cols-1) * border\n",
        "  height = rows * k + (rows-1) * border\n",
        "  img = np.zeros([height, width, num_channels])\n",
        "  #img = np.zeros([height, width])\n",
        "  for i in range(num_filters):\n",
        "    r = int(i / cols) * (k + border)\n",
        "    c = int(i % cols) * (k + border)\n",
        "    img[r:r+k,c:c+k,:] = w[:,:,:,i]\n",
        "  filename = 'epoch_%02d_step_%06d.png' % (epoch, step)\n",
        "  if(img.shape[2]==1):\n",
        "      zipped = np.dstack((img, img, img))\n",
        "  else:\n",
        "      zipped = img\n",
        "  print(img.shape)\n",
        "  ski.io.imsave(os.path.join(save_dir, filename), zipped)\n",
        "\n",
        "\n",
        "  \n",
        "def plot_training_progress(save_dir, data):\n",
        "  fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,8))\n",
        "\n",
        "  linewidth = 2\n",
        "  legend_size = 10\n",
        "  train_color = 'm'\n",
        "  val_color = 'c'\n",
        "\n",
        "  num_points = len(data['train_loss'])\n",
        "  x_data = np.linspace(1, num_points, num_points)\n",
        "  ax1.set_title('Cross-entropy loss')\n",
        "  ax1.plot(x_data, data['train_loss'], marker='o', color=train_color,\n",
        "           linewidth=linewidth, linestyle='-', label='train')\n",
        "  ax1.plot(x_data, data['valid_loss'], marker='o', color=val_color,\n",
        "           linewidth=linewidth, linestyle='-', label='validation')\n",
        "  ax1.legend(loc='upper right', fontsize=legend_size)\n",
        "  ax2.set_title('Average class accuracy')\n",
        "  ax2.plot(x_data, data['train_acc'], marker='o', color=train_color,\n",
        "           linewidth=linewidth, linestyle='-', label='train')\n",
        "  ax2.plot(x_data, data['valid_acc'], marker='o', color=val_color,\n",
        "           linewidth=linewidth, linestyle='-', label='validation')\n",
        "  ax2.legend(loc='upper left', fontsize=legend_size)\n",
        "  ax3.set_title('Learning rate')\n",
        "  ax3.plot(x_data, data['lr'], marker='o', color=train_color,\n",
        "           linewidth=linewidth, linestyle='-', label='learning_rate')\n",
        "  ax3.legend(loc='upper left', fontsize=legend_size)\n",
        "\n",
        "  save_path = os.path.join(save_dir, 'training_plot.pdf')\n",
        "  print('Plotting in: ', save_path)\n",
        "  plt.savefig(save_path)\n",
        "\n",
        "  \n",
        "def class_to_onehot(Y):\n",
        "  Yoh=np.zeros((len(Y),max(Y)+1))\n",
        "  Yoh[range(len(Y)),Y] = 1\n",
        "  return Yoh\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TFConv2d_CIFAR:\n",
        "    \n",
        "    def __init__(self,width=1, height=1, channels=1):\n",
        "        \n",
        "        \n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.channels = channels\n",
        "        \n",
        "        self.conv1_fmaps = 16\n",
        "        self.conv1_ksize = 5\n",
        "        self.conv1_stride = 1\n",
        "        self.conv1_pad = \"SAME\"\n",
        "        \n",
        "        self.conv2_fmaps = 32\n",
        "        self.conv2_ksize = 5\n",
        "        self.conv2_stride = 1\n",
        "        self.conv2_pad = \"SAME\"\n",
        "        \n",
        "        pool3_fmaps = conv2_fmaps\n",
        "        learning_rate = 0.001\n",
        "\n",
        "        n_fc1 = 256\n",
        "        n_fc2 = 128\n",
        "        n_outputs = 10\n",
        "        \n",
        "        self.X = tf.palceholder(tf.float32, shape=[None, self.width * self.height], name=\"X\")\n",
        "        self.X_reshaped = tf.reshape(self.X, shape=[-1, self.height, self.width, self.channels])\n",
        "        self.y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
        "\n",
        "        l2_reg = 0.001\n",
        "        regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
        "               \n",
        "        self.conv1 = tf.layers.conv2d(self.X_reshaped, filters= conv1_fmaps, kernel_size = conv1_ksize\n",
        "                                ,strides = conv1_stride, padding = conv1_pad, kernel_regularizer = None,\n",
        "                                activation=tf.nn.relu, name=\"conv1\")\n",
        "        self.max_pool1 = tf.nn.max_pool(self.conv1, ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"VALID\")\n",
        "        \n",
        "        \n",
        "        self.conv2 = tf.layers.conv2d(self.max_pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
        "                                strides=conv2_stride, padding=conv2_pad, kernel_regularizer = None,\n",
        "                                activation=tf.nn.relu, name=\"conv2\")\n",
        "        \n",
        "        self.max_pool2 = tf.nn.max_pool(self.conv2, ksize=[1,3,3,1], strides=[1,2,2,1], padding=\"VALID\")\n",
        "        \n",
        "        self.pool2_flattern = tf.reshape(self.max_pool2, shape=[-1, pool3_fmaps *8*8])\n",
        "        \n",
        "        self.fc1 = tf.layers.dense(self.pool2_flattern, n_fc1, activation=tf.nn.relu,kernel_regularizer = None,\n",
        "                                  name=\"fc1\")\n",
        "        \n",
        "        self.fc2 = tf.layers.dense(self.fc1, n_fc2, activation=tf.nn.relu, kernel_regularizer = None,\n",
        "                                  name=\"fc2\")\n",
        "        \n",
        "        self.logits = tf.layers.dense(self.fc2, n_outputs, name=\"output\")\n",
        "        self.Y_proba = tf.nn.softmax(self.logits, name=\"Y_proba\")\n",
        "        \n",
        "        self.xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
        "                                                                  labels=y_train)\n",
        "        self.loss = tf.reduce_mean(self.xentropy)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        self.training_op = self.optimizer.minimize(self.loss)\n",
        "        \n",
        "        self.correct = tf.nn.in_top_k(self.logits, self.y, 1)\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(self.correct, tf.float32))\n",
        "        \n",
        "        self.init = tf.global_variables_initializer()\n",
        "        self.saver = tf.train.Saver()\n",
        "        self.sess = tf.Session()\n",
        "    \n",
        "    def evaluate(self, X_eval, y_true, my_title=\"train\"):\n",
        "        print(\"Usao sam\")\n",
        "        y_pred = np.argmax(self.prediction(X_eval),axis=1)\n",
        "        cnf_matrix = confusion_matrix(y_true, y_pred)\n",
        "        cnn_recall = recall_score(y_true, y_pred, average='micro')\n",
        "        cnn_precision = precision_score(y_true, y_pred, average='micro')\n",
        "        cnn_acc = accuracy_score(y_true, y_pred)\n",
        "        print(my_title)\n",
        "        print(cnf_matrix)\n",
        "        return  cnn_acc, cnn_precision, cnn_recall\n",
        "    \n",
        "    def prediction(self, X_eval):\n",
        "        sess = self.sess\n",
        "#             self.saver.restore(sess, \"./batch_conv2d_para.ckpt\")\n",
        "#         sess.run(self.init)\n",
        "        return sess.run(self.Y_proba, feed_dict={self.X: X_eval})\n",
        "    \n",
        "    def draw_conv_filters(self, weights):\n",
        "        w = weights.copy()\n",
        "        num_filters = w.shape[3]\n",
        "        num_channels = w.shape[2]\n",
        "        k = w.shape[0]\n",
        "        assert w.shape[0] == w.shape[1]\n",
        "        w = w.reshape(k, k, num_channels, num_filters)\n",
        "        w -= w.min()\n",
        "        w /= w.max()\n",
        "        border = 1\n",
        "        cols = 8\n",
        "        rows = math.ceil(num_filters / cols)\n",
        "        width = cols * k + (cols-1) * border\n",
        "        height = rows * k + (rows-1) * border\n",
        "        img = np.zeros([np.int(height), np.int(width), np.int(num_channels)])\n",
        "        for i in range(np.int(num_filters)):\n",
        "            r = int(i / cols) * (k + border)\n",
        "            c = int(i % cols) * (k + border)\n",
        "            img[r:r+k,c:c+k,:] = w[:,:,:,i]\n",
        "#         img.astype(np.uint8)\n",
        "#         ski.io.imshow(img)\n",
        "#         ski.io.show()\n",
        "        print(\"Shape is \", img.shape)\n",
        "        imgplot = plt.imshow(img.squeeze())\n",
        "        plt.show()\n",
        "  \n",
        "    \n",
        "    def visual_con(self):\n",
        "        conv1_var = tf.contrib.framework.get_variables('my_conv1/kernel:0')[0]\n",
        "        conv1_weights = conv1_var.eval(session=self.sess)\n",
        "        self.draw_conv_filters(conv1_weights)\n",
        "  \n",
        "  \n",
        "    def batch_train(self, X, y, niter=10000, batch_size = 50):\n",
        "        sess = self.sess\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0, test_size=0.1)\n",
        "        \n",
        "        n, N= X_train.shape\n",
        "        s = np.arange(n)\n",
        "        \n",
        "        ako = True\n",
        "        acc_train = []\n",
        "        recall_train = []\n",
        "        precision_train = []\n",
        "        acc_valid = []\n",
        "        recall_valid = []\n",
        "        precision_valid = []\n",
        "        \n",
        "        max_loss1 = 0\n",
        "        max_loss2 = 0\n",
        "        max_loss3 = 0\n",
        "        X1 = None\n",
        "        X2 = None\n",
        "        X3 = None\n",
        "        y1 = None\n",
        "        y2 = None\n",
        "        y3 = None\n",
        "        \n",
        "        if ako:\n",
        "            sess.run(self.init)\n",
        "    \n",
        "            for i in range(niter):\n",
        "            \n",
        "                n_batches = n // batch_size\n",
        "                np.random.shuffle(s)\n",
        "                X_train = X_train[s]\n",
        "                y_train = y_train[s]\n",
        "                \n",
        "                for j in range(n_batches):\n",
        "                    X_batch = X_train[j*batch_size: (j+1)*batch_size]\n",
        "                    y_batch = y_train[j*batch_size: (j+1)*batch_size]\n",
        "                    \n",
        "                    _, loss = sess.run([self.training_op, self.loss], feed_dict={self.X: X_batch, self.y: y_batch})\n",
        "                    \n",
        "                    if(loss > max_loss1):\n",
        "                        X1 = X_batch\n",
        "                        y1 = y_batch\n",
        "                        max_loss1 = loss\n",
        "                    elif(loss > max_loss2):\n",
        "                        X2 = X_batch\n",
        "                        y2 = y_batch\n",
        "                        max_loss2 = loss\n",
        "                    elif(loss > max_loss3):\n",
        "                        X3 = X_batch\n",
        "                        y3 = y_batch\n",
        "                        max_loss3 = loss\n",
        "\n",
        "#                 cnn_acc_train, cnn_precision_train, cnn_recall_train= self.evaluate(X_train, y_train)\n",
        "#                 acc_train += [cnn_acc_train]\n",
        "#                 recall_train += [cnn_recall_train]\n",
        "#                 precision_train += [cnn_precision_train]\n",
        "                cnn_acc_valid, cnn_precision_valid, cnn_recall_valid= self.evaluate(X_valid, y_valid, my_title=\"valid\")\n",
        "                acc_valid += [cnn_acc_valid]\n",
        "                recall_valid += [cnn_recall_valid]\n",
        "                precision_valid += [cnn_precision_valid]\n",
        "            self.visual_con()\n",
        "            print(\"First loss is\",X1)\n",
        "            print(\"Predictio is\", self.prediction(X1))\n",
        "            print(\"Real class is\", y1)\n",
        "            print(\"First loss is\",X2)\n",
        "            print(\"Predictio is\", self.prediction(X2))\n",
        "            print(\"Real class is\", y2)\n",
        "            print(\"First loss is\",X3)\n",
        "            print(\"Predictio is\", self.prediction(X3))\n",
        "            print(\"Real class is\", y3)\n",
        "#         fig, axes = plt.subplots(6,1,figsize=(20, 10))\n",
        "#         axes[0].plot(np.linspace(1.0, niter,num=niter), acc_train, label='accuracy train')\n",
        "#         axes[0].legend(loc='best')\n",
        "#         axes[0].set_xlabel('number of iteration')\n",
        "\n",
        "\n",
        "\n",
        "#         axes[1].plot(np.linspace(1.0, niter,num=niter), recall_train, label= 'recall train')\n",
        "\n",
        "\n",
        "#         axes[1].legend(loc='best')\n",
        "#         axes[1].set_xlabel('number of iteration')\n",
        "        \n",
        "#         axes[2].plot(np.linspace(1.0, niter,num=niter), cnn_precision_train, label='precision train')\n",
        "#         axes[2].legend(loc='best')\n",
        "#         axes[2].set_xlabel('number of iteration')\n",
        "        \n",
        "#         axes[3].plot(np.linspace(1.0, niter,num=niter), acc_valid, label='accuracy valid')\n",
        "#         axes[3].legend(loc='best')\n",
        "#         axes[3].set_xlabel('number of iteration')\n",
        "\n",
        "\n",
        "\n",
        "#         axes[4].plot(np.linspace(1.0, niter,num=niter), recall_valid, label= 'recall valid')\n",
        "\n",
        "\n",
        "#         axes[4].legend(loc='best')\n",
        "#         axes[4].set_xlabel('number of iteration')\n",
        "        \n",
        "#         axes[5].plot(np.linspace(1.0, niter,num=niter), cnn_precision_valid, label='precision valid')\n",
        "#         axes[5].legend(loc='best')\n",
        "#         axes[5].set_xlabel('number of iteration')\n",
        "\n",
        "#         plt.show()\n",
        "    \n",
        "    \n",
        "   \n",
        "if __name__==\"__main__\":\n",
        "    np.random.seed(100)\n",
        "    tf.set_random_seed(100)\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    config = {}\n",
        "    config['max_epochs'] = 8\n",
        "    config['batch_size'] = 50\n",
        "    config['save_dir'] = SAVE_DIR\n",
        "    config['lr_policy'] = {1:{'lr':1e-1}, 3:{'lr':1e-2}, 5:{'lr':1e-3}, 7:{'lr':1e-4}}\n",
        "    \n",
        "    img_height, img_width = 32, 32\n",
        "    num_channels = 3\n",
        "    num_classes = 10\n",
        "    \n",
        "    train_x = np.ndarray((0, img_height * img_width * num_channels), dtype=np.float32)\n",
        "    train_y = []\n",
        "    for i in range(1, 6):\n",
        "      subset = unpickle(os.path.join(DATA_DIR, 'data_batch_%d' % i))\n",
        "      train_x = np.vstack((train_x, subset['data']))\n",
        "      train_y += subset['labels']\n",
        "    train_x = train_x.reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1)\n",
        "    train_y = np.array(train_y, dtype=np.int32)\n",
        "    \n",
        "    subset = unpickle(os.path.join(DATA_DIR, 'test_batch'))\n",
        "    test_x = subset['data'].reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1).astype(np.float32)\n",
        "    test_y = np.array(subset['labels'], dtype=np.int32)\n",
        "    \n",
        "    valid_size = 5000\n",
        "    train_x, train_y = shuffle_data(train_x, train_y)\n",
        "    valid_x = train_x[:valid_size, ...]\n",
        "    valid_y = train_y[:valid_size, ...]\n",
        "    train_x = train_x[valid_size:, ...]\n",
        "    train_y = train_y[valid_size:, ...]\n",
        "    data_mean = train_x.mean((0,1,2))\n",
        "    data_std = train_x.std((0,1,2))\n",
        "    \n",
        "    train_x = (train_x - data_mean) / data_std\n",
        "    valid_x = (valid_x - data_mean) / data_std\n",
        "    test_x = (test_x - data_mean) / data_std\n",
        "\n",
        "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
        "    X = tf.placeholder(tf.float32,[None,img_height,img_width,num_channels])\n",
        "    Y = tf.placeholder(tf.float32,[None,num_classes])\n",
        "\n",
        "#     train_y = class_to_onehot(train_y)\n",
        "#     valid_y = class_to_onehot(valid_y)\n",
        "#     test_y = class_to_onehot(test_y)\n",
        "    \n",
        "    my_cifra_cnn = TFConv2d_CIFAR(width=img_width, height=img_height, channels=num_channels)\n",
        "    my_cifra_nn.batch_train(train_x,train_y, niter=2)\n",
        "    \n",
        "#     logits, loss = build_model(X, Y, num_classes)\n",
        "    \n",
        "#     trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "#     train_op = trainer.minimize(loss)\n",
        "    \n",
        "#     sess = tf.Session()\n",
        "\n",
        "#     run_ops = [logits, loss, train_op]\n",
        "#     logits,loss = train(train_x, train_y, valid_x, valid_y, run_ops, config, sess)\n",
        "#     # here it dies\n",
        "#     run_ops = [logits, loss]\n",
        "#     evaluate(\"Test\", test_x, test_y, config, run_ops, sess)\n",
        "        "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-b1d928d00b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m       \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_batch_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m       \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0mtrain_y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-b1d928d00b5a>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, encoding='latin1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '\\x1f'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "C2WgrSJRbnqC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "981e7a2c-1043-411d-9af6-1b8eab197af8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523883621300,
          "user_tz": -120,
          "elapsed": 578,
          "user": {
            "displayName": "Branko Radoš",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100447339136947181118"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f for f in listdir(\"./\") if isfile(join(\"./\", f))]\n",
        "print(onlyfiles)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['batch_conv2d_para.ckpt.meta', 'cifar-10-python', 'batch_conv2d_para.ckpt.data-00000-of-00001', 'checkpoint', 'data_batch_1', '.rnd', 'batch_conv2d_para.ckpt.index', 'cifar-10-python.tar']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7VH3-W6Wfz6w",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "48f7646e-35f5-4e6c-947c-b08ec8a7ccda",
        "executionInfo": {
          "status": "error",
          "timestamp": 1523883215808,
          "user_tz": -120,
          "elapsed": 26594,
          "user": {
            "displayName": "Branko Radoš",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100447339136947181118"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import urllib2\n",
        "import StringIO\n",
        "import gzip\n",
        "\n",
        "baseURL = \"https://www.cs.toronto.edu/~kriz/\"\n",
        "filename = \"cifar-10-python.tar.gz\"\n",
        "outFilePath = \"./\"\n",
        "\n",
        "response = urllib2.urlopen(baseURL + filename)\n",
        "compressedFile = StringIO.StringIO()\n",
        "compressedFile.write(response.read())\n",
        "#\n",
        "# Set the file's current position to the beginning\n",
        "# of the file so that gzip.GzipFile can read\n",
        "# its contents from the top.\n",
        "#\n",
        "compressedFile.seek(0)\n",
        "\n",
        "decompressedFile = gzip.GzipFile(fileobj=compressedFile, mode='rb')\n",
        "\n",
        "with open(outFilePath, 'w') as outfile:\n",
        "    outfile.write(decompressedFile.read())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IOError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-640a149a53c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdecompressedFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompressedFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutFilePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressedFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIOError\u001b[0m: [Errno 21] Is a directory: './'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oZpmfx6viWl-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "global dump\n",
        "\n",
        "def download_file():\n",
        "    global dump\n",
        "    url = \"https://drive.google.com/open?id=1Boewt6HNcdLFpUx_AHe12TXTNGY7ZS0Z\"\n",
        "    file = requests.get(url, stream=True)\n",
        "    dump = file.raw\n",
        "\n",
        "def save_file():\n",
        "    global dump\n",
        "    location = os.path.abspath(\"./\")\n",
        "    with open(\"test_batch\", 'wb') as location:\n",
        "        shutil.copyfileobj(dump, location)\n",
        "    del dump\n",
        "    \n",
        "download_file()\n",
        "save_file()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2KEzwowUl0jy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "a70fd20d-8fb2-4d3c-ef58-6c8d67f7dd87",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1523886014118,
          "user_tz": -120,
          "elapsed": 1551602,
          "user": {
            "displayName": "Branko Radoš",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100447339136947181118"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-51e370ac-2621-4ba0-92a2-7c089f1cba59\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-51e370ac-2621-4ba0-92a2-7c089f1cba59\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving batches.meta to batches.meta\n",
            "Saving data_batch_1 to data_batch_1 (1)\n",
            "Saving data_batch_2 to data_batch_2 (1)\n",
            "Saving data_batch_3 to data_batch_3 (1)\n",
            "Saving data_batch_4 to data_batch_4 (1)\n",
            "Saving data_batch_5 to data_batch_5 (1)\n",
            "Saving readme.html to readme.html\n",
            "Saving test_batch to test_batch (1)\n",
            "User uploaded file \"data_batch_1\" with length 31035704 bytes\n",
            "User uploaded file \"data_batch_2\" with length 31035320 bytes\n",
            "User uploaded file \"data_batch_3\" with length 31035999 bytes\n",
            "User uploaded file \"data_batch_4\" with length 31035696 bytes\n",
            "User uploaded file \"batches.meta\" with length 158 bytes\n",
            "User uploaded file \"test_batch\" with length 31035526 bytes\n",
            "User uploaded file \"readme.html\" with length 88 bytes\n",
            "User uploaded file \"data_batch_5\" with length 31035623 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}